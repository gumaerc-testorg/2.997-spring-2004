---
content_type: page
description: This page includes reading assignments.
draft: false
learning_resource_types:
- Readings
ocw_type: CourseSection
title: Readings
uid: 6897a3ff-dccb-36d9-9430-7f489f653321
---
Bertsekas = Bertsekas, Dimitri P. *Dynamic Programming and Optimal Control*. 2 vols. Belmont, MA: Athena Scientific, 2007. ISBN: 9781886529083.

Bertsekas and Tsitsiklis = Bertsekas, Dimitri P., and John N. Tsitsiklis. *Neuro-Dynamic Programming*. Belmont, MA: Athena Scientific, 1996. ISBN: 9781886529106.

{{< tableopen >}}{{< theadopen >}}{{< tropen >}}{{< thopen >}}
LECÂ #
{{< thclose >}}{{< thopen >}}
TOPICS
{{< thclose >}}{{< thopen >}}
READINGS
{{< thclose >}}{{< trclose >}}{{< theadclose >}}{{< tbodyopen >}}{{< tropen >}}{{< tdopen >}}
1
{{< tdclose >}}{{< tdopen >}}

Markov Decision Processes

Finite-Horizon Problems: Backwards Induction

Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation

{{< tdclose >}}{{< tdopen >}}
Bertsekas Vol. 1, Chapter 1.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
2
{{< tdclose >}}{{< tdopen >}}

Value Iteration

Existence and Uniqueness of Bellman's Equation Solution

Gauss-Seidel Value Iteration

{{< tdclose >}}{{< tdopen >}}

Bertsekas Vol. 2, Chapter 1.

Bertsekas and Tsitsiklis, Chapter 2.

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
3
{{< tdclose >}}{{< tdopen >}}

Optimality of Policies derived from the Cost-to-Go Function

Policy Iteration

Asynchronous Policy Iteration

{{< tdclose >}}{{< tdopen >}}

Bertsekas Vol. 2, Chapter 1.

Bertsekas and Tsitsiklis, Chapter 2.

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
4
{{< tdclose >}}{{< tdopen >}}

Average-Cost Problems

Relationship with Discounted-Cost Problems

Bellman's Equation

Blackwell Optimality

{{< tdclose >}}{{< tdopen >}}
Bertsekas Vol. 2, Chapter 4.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
5
{{< tdclose >}}{{< tdopen >}}

Average-Cost Problems

Computational Methods

{{< tdclose >}}{{< tdopen >}}
Bertsekas Vol. 2, Chapter 4.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
6
{{< tdclose >}}{{< tdopen >}}

Application of Value Iteration to Optimization of Multiclass Queueing Networks

Introduction to Simulation-based Methods Real-Time Value Iteration

{{< tdclose >}}{{< tdopen >}}

Chen, R. R., and S. P. Meyn. "{{% resource_link "b59f9e8c-02fd-4ceb-ac52-70b00cd7f52b" "Value Iteration and Optimization of Multiclass Queueing Networks" %}}."*Queueing Systems* 32 (1999): 65-97.

Bertsekas and Tsitsiklis, Chapter 5.

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
7
{{< tdclose >}}{{< tdopen >}}

Q-Learning

Stochastic Approximations

{{< tdclose >}}{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapters 4 and 5.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
8
{{< tdclose >}}{{< tdopen >}}

Stochastic Approximations: Lyapunov Function Analysis

The ODE Method

Convergence of Q-Learning

{{< tdclose >}}{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapters 4 and 5.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
9
{{< tdclose >}}{{< tdopen >}}
Exploration versus Exploitation: The Complexity of Reinforcement Learning
{{< tdclose >}}{{< tdopen >}}
Kearns, M. , and S. Singh. "{{% resource_link "7ddd77a9-1ed0-4651-abd6-03771ae133a5" "Near-Optional Reinforcement Learning in Polynomial Time" %}}." *Machine Learning* 49, no. 2 (Nov 2002): 209-232.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
10
{{< tdclose >}}{{< tdopen >}}

Introduction to Value Function Approximation

Curse of Dimensionality

Approximation Architectures

{{< tdclose >}}{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
11
{{< tdclose >}}{{< tdopen >}}
Model Selection and Complexity
{{< tdclose >}}{{< tdopen >}}
Hastie, Tibshirani, and Friedmann. Chapter 7 in *The Elements of Statistical Learning*. New York: Springer, 2003. ISBN: 9780387952840.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
12
{{< tdclose >}}{{< tdopen >}}

Introduction to Value Function Approximation Algorithms

Performance Bounds

{{< tdclose >}}{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
13
{{< tdclose >}}{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation
{{< tdclose >}}{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
14
{{< tdclose >}}{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)
{{< tdclose >}}{{< tdopen >}}

Bertsekas and Tsitsiklis, Chapter 6.

de Farias, D. P., and B. Van Roy. "{{% resource_link "8db27685-c343-431c-9ab1-122682ae2aca" "On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning" %}}."

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
15
{{< tdclose >}}{{< tdopen >}}

Temporal-Difference Learning with Value Function Approximation (cont.)

Optimal Stopping Problems

General Control Problems

{{< tdclose >}}{{< tdopen >}}

Bertsekas and Tsitsiklis, Chapter 6.

de Farias, D. P., and B. Van Roy. "{{% resource_link "8db27685-c343-431c-9ab1-122682ae2aca" "On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning" %}}."

Bertsekas, Borkar, and Nedic. "{{% resource_link "31eae135-ded3-4ff9-9405-bbfe087eabb9" "Improved temporal Difference Methods with Linear Function Approximation" %}}."

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
16
{{< tdclose >}}{{< tdopen >}}
Approximate Linear Programming
{{< tdclose >}}{{< tdopen >}}
de Farias, D. P., and B. Van Roy. "{{% resource_link "5b344dfd-81be-45f4-80e6-d9033b834e9f" "The Linear Programming Approach to Approximate Dynamic Programming" %}}."
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
17
{{< tdclose >}}{{< tdopen >}}
Approximate Linear Programming (cont.)
{{< tdclose >}}{{< tdopen >}}
de Farias, D. P., and B. Van Roy. "{{% resource_link "5b344dfd-81be-45f4-80e6-d9033b834e9f" "The Linear Programming Approach to Approximate Dynamic Programming" %}}."
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
18
{{< tdclose >}}{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming
{{< tdclose >}}{{< tdopen >}}

de Farias D. P., and B. Van Roy. "{{% resource_link "5f2290a3-668e-4b56-8f58-07387bf07152" "On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming" %}}."

Calafiori, and Campi. "{{% resource_link "8fe6eb98-bf03-469b-8c4b-fc7cda26a0ed" "Uncertain Convex Programs: Randomized Solutions and Confidence Levels" %}}."

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
19
{{< tdclose >}}{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming: Factored MDPs
{{< tdclose >}}{{< tdopen >}}

Guestrin, et al. "{{% resource_link "d171be9c-2bb1-4e05-b99b-17cfe00561b9" "Efficient Solution Algorithms for Factored MDPs" %}}."

Schuurmans, and Patrascu. "{{% resource_link "c7918fc1-f384-4b64-a982-7d002d37c3ac" "Direct Value Approximation for Factored MDPs" %}}."

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
20
{{< tdclose >}}{{< tdopen >}}
Policy Search Methods
{{< tdclose >}}{{< tdopen >}}
Marbach, and Tsitsiklis. "Simulation-Based Optimization of Markov Reward Processes." ({{% resource_link "965e543f-dbd2-417f-8456-305724366016" "PDF" %}})
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
21
{{< tdclose >}}{{< tdopen >}}
Policy Search Methods (cont.)
{{< tdclose >}}{{< tdopen >}}
Baxter, and Bartlett. "{{% resource_link "608086f8-de69-42c6-a070-5f611b5965b5" "Infinite-Horizon Policy-Gradient Estimation" %}}."
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
22
{{< tdclose >}}{{< tdopen >}}

Policy Search Methods for POMDPs

Application: Call Admission Control

Actor-Critic Methods

{{< tdclose >}}{{< tdopen >}}

Baxter, and Bartlett. "{{% resource_link "608086f8-de69-42c6-a070-5f611b5965b5" "Infinite-Horizon Policy-Gradient Estimation" %}}."

Baxter, and Bartlett. "{{% resource_link "c241a933-703e-4281-a04a-9939c0bebc26" "Experiments with Infinite-Horizon Policy-Gradient Estimation" %}}."

Konda, and Tsitsiklis. "Actor-Critic Algorithms." ({{% resource_link "8c899ce5-cb3c-41cc-b6ff-a524641ad0d7" "PDF" %}})

{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
23
{{< tdclose >}}{{< tdopen >}}

Guest Lecture: Prof. Nick Roy

Approximate POMDP Compression

{{< tdclose >}}{{< tdopen >}}
Roy, and Gordon. "{{% resource_link "2d08de83-ea03-4f9c-8e33-eb29f778c267" "Exponential Family PCA for Belief Compression in POMDPs" %}}."
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
24
{{< tdclose >}}{{< tdopen >}}

Policy Search Methods: PEGASUS

Application: Helicopter Control

{{< tdclose >}}{{< tdopen >}}

Ng, and Jordan. "{{% resource_link "6a0fc869-64f0-4f68-bb0b-178876a2cbfb" "PEGASUS: A policy search method for large MDPs and POMDPs" %}}."

Ng, et al. "{{% resource_link "30ffcb08-b9f5-4c72-b9b7-5f90b2ee3bbb" "Autonomous Helicopter Flight via Reinforcement Learning" %}}."

{{< tdclose >}}{{< trclose >}}{{< tbodyclose >}}{{< tableclose >}}

## Complementary Reading

Even-Dar, and Mansour. "{{% resource_link "84826a08-1fda-4f76-ace7-58e7f973a231" "Learning Rates for Q-Learning" %}}.'' *Journal of Machine Learning Research* 5 (2003): 1-25.

Barron. "Universal approximation bounds for superpositions of a sigmoidal function." *IEEE Transactions on Information Theory* 39 (1993): 930-944.

Tesauro. "{{% resource_link "72e3ff44-f00b-4f95-bf3d-5a7ad42b63c5" "Temporal-Difference Learning and TD-Gammon" %}}'' *Communications of the ACM* 38, no. 3 (1995).
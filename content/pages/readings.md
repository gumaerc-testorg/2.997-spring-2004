---
content_type: page
description: ''
learning_resource_types:
- Readings
ocw_type: CourseSection
title: Readings
uid: 6897a3ff-dccb-36d9-9430-7f489f653321
---

Bertsekas = Bertsekas, Dimitri P. _Dynamic Programming and Optimal Control_. 2 vols. Belmont, MA: Athena Scientific, 2007. ISBN: 9781886529083.

Bertsekas and Tsitsiklis = Bertsekas, Dimitri P., and John N. Tsitsiklis. _Neuro-Dynamic Programming_. Belmont, MA: Athena Scientific, 1996. ISBN: 9781886529106.

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
LECÂ #
{{< thclose >}}
{{< thopen >}}
TOPICS
{{< thclose >}}
{{< thopen >}}
READINGS
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}
Markov Decision Processes  
  
Finite-Horizon Problems: Backwards Induction  
  
Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 1, Chapter 1.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}
Value Iteration  
  
Existence and Uniqueness of Bellman's Equation Solution  
  
Gauss-Seidel Value Iteration
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 1.  
  
Bertsekas and Tsitsiklis, Chapter 2.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}
Optimality of Policies derived from the Cost-to-Go Function  
  
Policy Iteration  
  
Asynchronous Policy Iteration
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 1.  
  
Bertsekas and Tsitsiklis, Chapter 2.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Relationship with Discounted-Cost Problems  
  
Bellman's Equation  
  
Blackwell Optimality
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 4.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Computational Methods
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 4.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}
Application of Value Iteration to Optimization of Multiclass Queueing Networks  
  
Introduction to Simulation-based Methods Real-Time Value Iteration
{{< tdclose >}}
{{< tdopen >}}
Chen, R. R., and S. P. Meyn. "{{% resource_link "9af3f3d5-28e1-4709-bf37-dd7a630337ea" "Value Iteration and Optimization of Multiclass Queueing Networks" %}}."_Queueing Systems_ 32 (1999): 65-97.  
  
Bertsekas and Tsitsiklis, Chapter 5.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}
Q-Learning  
  
Stochastic Approximations
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapters 4 and 5.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}
Stochastic Approximations: Lyapunov Function Analysis  
  
The ODE Method  
  
Convergence of Q-Learning
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapters 4 and 5.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}
Exploration versus Exploitation: The Complexity of Reinforcement Learning
{{< tdclose >}}
{{< tdopen >}}
Kearns, M. , and S. Singh. "{{% resource_link "70860076-ae65-484d-be3c-81ae0bca2991" "Near-Optional Reinforcement Learning in Polynomial Time" %}}." _Machine Learning_ 49, no. 2 (Nov 2002): 209-232.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation  
  
Curse of Dimensionality  
  
Approximation Architectures
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}
Model Selection and Complexity
{{< tdclose >}}
{{< tdopen >}}
Hastie, Tibshirani, and Friedmann. Chapter 7 in _The Elements of Statistical Learning_. New York: Springer, 2003. ISBN: 9780387952840.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation Algorithms  
  
Performance Bounds
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.  
  
de Farias, D. P., and B. Van Roy. "{{% resource_link "d651a13e-d5ff-4858-a093-039ac44382cf" "On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)  
  
Optimal Stopping Problems  
  
General Control Problems
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.  
  
de Farias, D. P., and B. Van Roy. "{{% resource_link "d651a13e-d5ff-4858-a093-039ac44382cf" "On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning" %}}."  
  
Bertsekas, Borkar, and Nedic. "{{% resource_link "97186eb8-c4f7-4ee8-a8f8-13ad8d56bfb3" "Improved temporal Difference Methods with Linear Function Approximation" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}
de Farias, D. P., and B. Van Roy. "{{% resource_link "36991259-8c1f-44fb-b718-6122641b39a9" "The Linear Programming Approach to Approximate Dynamic Programming" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming (cont.)
{{< tdclose >}}
{{< tdopen >}}
de Farias, D. P., and B. Van Roy. "{{% resource_link "36991259-8c1f-44fb-b718-6122641b39a9" "The Linear Programming Approach to Approximate Dynamic Programming" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}
de Farias D. P., and B. Van Roy. "{{% resource_link "046caf5c-daef-463a-8411-536338f0d291" "On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming" %}}."  
  
Calafiori, and Campi. "{{% resource_link "838b6a0e-fcc4-40e1-a9fa-be7111d52e11" "Uncertain Convex Programs: Randomized Solutions and Confidence Levels" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming: Factored MDPs
{{< tdclose >}}
{{< tdopen >}}
Guestrin, et al. "{{% resource_link "4b34cc79-4446-4dca-bd55-202f8390b3a5" "Efficient Solution Algorithms for Factored MDPs" %}}."  
  
Schuurmans, and Patrascu. "{{% resource_link "9d9604e9-6ada-40af-b68a-39b8608601a5" "Direct Value Approximation for Factored MDPs" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods
{{< tdclose >}}
{{< tdopen >}}
Marbach, and Tsitsiklis. "Simulation-Based Optimization of Markov Reward Processes." ({{% resource_link "bc0b596f-23ac-4b7b-8724-ab6e213827c9" "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
21
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods (cont.)
{{< tdclose >}}
{{< tdopen >}}
Baxter, and Bartlett. "{{% resource_link "11451a5f-2d68-4363-b767-bc59a55a5273" "Infinite-Horizon Policy-Gradient Estimation" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
22
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods for POMDPs  
  
Application: Call Admission Control  
  
Actor-Critic Methods
{{< tdclose >}}
{{< tdopen >}}
Baxter, and Bartlett. "{{% resource_link "11451a5f-2d68-4363-b767-bc59a55a5273" "Infinite-Horizon Policy-Gradient Estimation" %}}."  
  
Baxter, and Bartlett. "{{% resource_link "46001621-912e-4ce7-a2c8-853aaf21c912" "Experiments with Infinite-Horizon Policy-Gradient Estimation" %}}."  
  
Konda, and Tsitsiklis. "Actor-Critic Algorithms." ({{% resource_link "633677bd-f7e8-4be0-ac2e-1fbfeeee0862" "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
23
{{< tdclose >}}
{{< tdopen >}}
Guest Lecture: Prof. Nick Roy  
  
Approximate POMDP Compression
{{< tdclose >}}
{{< tdopen >}}
Roy, and Gordon. "{{% resource_link "4462e978-5732-44ab-bcc5-5579f4a99116" "Exponential Family PCA for Belief Compression in POMDPs" %}}."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
24
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods: PEGASUS  
  
Application: Helicopter Control
{{< tdclose >}}
{{< tdopen >}}
Ng, and Jordan. "{{% resource_link "31ed2273-1c34-4dae-bf10-83cbd9babdbe" "PEGASUS: A policy search method for large MDPs and POMDPs" %}}."  
  
Ng, et al. "{{% resource_link "78be6c13-3363-4b85-bf59-c0e4b1cec0db" "Autonomous Helicopter Flight via Reinforcement Learning" %}}."
{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}

Complementary Reading
---------------------

Even-Dar, and Mansour. "{{% resource_link "0e8710a7-c67d-48d8-97ce-6224969784ef" "Learning Rates for Q-Learning" %}}.'' _Journal of Machine Learning Research_ 5 (2003): 1-25.

Barron. "Universal approximation bounds for superpositions of a sigmoidal function." _IEEE Transactions on Information Theory_ 39 (1993): 930-944.

Tesauro. "{{% resource_link "76826e39-75e0-473c-b984-b061cb5fbaf1" "Temporal-Difference Learning and TD-Gammon" %}}'' _Communications of the ACM_ 38, no. 3 (1995).